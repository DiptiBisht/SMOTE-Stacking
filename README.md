# SMOTE-Stacking
Used a technique to handle imbalanced data - SMOTE, Ensemble predictions (stacking) using different types of prediction models, for Kaggle's Homesite Conversion dataset
(1) Experiment with SMOTE (or its variations) using different percentages to get a higher accuracy on minority class prediction.
(2) Perform ensemble predictions (one-layer stacking) by combining predictions from the various algorithms. For stacking, try at least five different models - e.g. decision tree, random forest, support vector machines, multilayer perceptron and K-nearest neighbors.
(3) In addition, perform hyperparameter tuning on the stacked model. You can do hyperparameter tuning on individual models if you want, but that is not necessary.
(4) Submit to Kaggle both individual and stacked model predictions and report all such Kaggle scores in an Excel table.
